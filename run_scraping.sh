#!/bin/bash
# Script para ejecutar el Enhanced Futurepedia Scraper

echo "=== Enhanced Futurepedia Scraper ==="
echo "Verificando configuraci√≥n del proyecto..."

#!/bin/bash

# Script para ejecutar el scraping de Futurepedia
# Modo profesional: scraping de todas las categor√≠as

set -e  # Detener si hay errores

echo "======================================"
echo "üöÄ FUTUREPEDIA AI TOOLS SCRAPER - MODO PROFESIONAL"
echo "======================================"

# Verificar dependencias
echo "üîç Verificando dependencias..."

# Verificar Python
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Python3 no est√° instalado"
    exit 1
fi

# Verificar archivo de requerimientos
if [ ! -f "requirements.txt" ]; then
    echo "‚ùå No se encuentra requirements.txt"
    exit 1
fi

# Verificar .env
if [ ! -f ".env" ]; then
    echo "‚ùå No se encuentra el archivo .env"
    echo "üìù Crea un archivo .env con las siguientes variables:"
    echo "FETCHFOX_API_KEY=tu_clave_aqui"
    echo "OPENROUTER_API_KEY=tu_clave_aqui"
    exit 1
fi

# Verificar archivo de categor√≠as
if [ ! -f "futurepeida_io_categories.csv" ]; then
    echo "‚ùå No se encuentra el archivo futurepeida_io_categories.csv"
    exit 1
fi

# Verificar estructura de directorios
echo "üìÅ Verificando estructura de directorios..."
mkdir -p outputs/{json,csv}
mkdir -p logs

echo "‚úÖ Dependencias verificadas"

# Mostrar informaci√≥n de categor√≠as
echo ""
echo "üìã INFORMACI√ìN DE CATEGOR√çAS:"
TOTAL_CATEGORIES=$(tail -n +2 futurepeida_io_categories.csv | wc -l)
echo "   - Total de categor√≠as a procesar: $TOTAL_CATEGORIES"
echo ""

# Ejecutar scraping
echo "üî• Iniciando scraping profesional de todas las categor√≠as..."
echo "üìÖ $(date)"
echo ""

# Parametros configurables
CSV_FILE="futurepeida_io_categories.csv"
MAX_WORKERS=12

echo "üìã Configuraci√≥n:"
echo "   - Archivo CSV: $CSV_FILE"
echo "   - Workers por categor√≠a: $MAX_WORKERS"
echo "   - Modo: Procesamiento de todas las categor√≠as"
echo ""

# Ejecutar el scraper en modo profesional
python3 scrape.py --all --csv "$CSV_FILE" --max-workers $MAX_WORKERS

SCRAPING_EXIT_CODE=$?

echo ""
echo "======================================"
if [ $SCRAPING_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ SCRAPING PROFESIONAL COMPLETADO EXITOSAMENTE"
    
    # Mostrar estad√≠sticas completas
    echo ""
    echo "üìä ESTAD√çSTICAS FINALES:"
    
    # Contar archivos JSON generados
    JSON_COUNT=$(find outputs/json -name "*.json" 2>/dev/null | wc -l)
    echo "   - Total herramientas scrapeadas: $JSON_COUNT"
    
    # Contar archivos CSV generados por categor√≠a
    CSV_COUNT=$(find outputs/csv -name "*.csv" 2>/dev/null | wc -l)
    echo "   - Categor√≠as procesadas (CSV): $CSV_COUNT"
    
    # Mostrar tama√±o de datos
    TOTAL_SIZE=$(du -sh outputs/ 2>/dev/null | cut -f1)
    echo "   - Tama√±o total de datos: $TOTAL_SIZE"
    
    # Mostrar archivos CSV generados
    echo ""
    echo "üìÅ ARCHIVOS CSV GENERADOS POR CATEGOR√çA:"
    find outputs/csv -name "*.csv" -type f -printf '%T@ %p
' 2>/dev/null | sort -nr | cut -d' ' -f2- | sed 's/^/     - /'
    
    # Mostrar resumen por categor√≠a si existe el log
    if [ -f "scraping.log" ]; then
        echo ""
        echo "üìà RESUMEN POR CATEGOR√çA:"
        echo "   (Ver scraping.log para detalles completos)"
        grep -i "√©xitos\|fallos\|total categor√≠as" scraping.log | tail -5 | sed 's/^/     /'
    fi
    
else
    echo "‚ùå ERROR EN EL SCRAPING PROFESIONAL (c√≥digo: $SCRAPING_EXIT_CODE)"
    echo "üìù Revisa scraping.log para m√°s detalles"
    
    # Mostrar √∫ltimos errores del log
    if [ -f "scraping.log" ]; then
        echo ""
        echo "üîç √öLTIMOS ERRORES:"
        grep -i "error\|fall√≥" scraping.log | tail -3 | sed 's/^/     /'
    fi
fi

echo ""
echo "üìù LOG COMPLETO: scraping.log"
echo "üïê Finalizado: $(date)"
echo "======================================"

echo "Directorio del proyecto verificado"

# Verificar dependencias de Python
echo " Verificando dependencias de Python..."
python3 -c "
import sys
missing = []
try:
    import requests
except ImportError:
    missing.append('requests')
try:
    import bs4
except ImportError:
    missing.append('beautifulsoup4')
try:
    import retrying
except ImportError:
    missing.append('retrying')
try:
    import fake_useragent
except ImportError:
    missing.append('fake-useragent')

if missing:
    print(f' Faltan dependencias: {missing}')
    print('   Instalando autom√°ticamente...')
    sys.exit(1)
else:
    print(' Todas las dependencias est√°n disponibles')
"

# Instalar dependencias si faltan
if [ $? -ne 0 ]; then
    echo "Instalando dependencias..."
    pip3 install -r requirements.txt
    if [ $? -ne 0 ]; then
        echo " Error instalando dependencias"
        exit 1
    fi
fi

# Verificar configuraci√≥n
echo " Verificando configuraci√≥n..."
if [ ! -f ".env" ]; then
    echo " Error: Archivo .env no encontrado"
    exit 1
fi

# Mostrar configuraci√≥n actual
echo "üìã Configuraci√≥n actual:"
source .env
echo "   - Delay entre requests: ${DELAY_BETWEEN_REQUESTS:-1.0}s"
echo "   - M√°ximo reintentos: ${MAX_RETRIES:-3}"
echo "   - Timeout: ${TIMEOUT:-30}s"
echo "   - FetchFox API: $([ -n "$FETCHFOX_API_KEY" ] && echo "‚úÖ Configurado" || echo "‚ùå No configurado (usar√° fallback)")"
echo "   - OpenRouter API: $([ -n "$OPENROUTER_API_KEY" ] && echo "‚úÖ Configurado" || echo "‚ùå No configurado (usar√° placeholders)")"

# Crear directorios necesarios
echo "üìÅ Creando directorios de salida..."
mkdir -p tools_data category_data

echo ""
echo "üöÄ Iniciando scraping del Enhanced Futurepedia Scraper..."
echo "   Categor√≠a objetivo: AI Personal Assistant Tools"
echo "   Modo: H√≠brido (FetchFox + BeautifulSoup)"
echo "   Concurrencia: 4 workers por defecto"
echo ""

# Ejecutar el scraper con la nueva implementaci√≥n
python3 scrape.py --category "AI Personal Assistant Tools" --max-workers 4

if [ $? -eq 0 ]; then
    echo ""
    echo "üéâ ¬°Scraping completado exitosamente!"
    echo ""
    echo "üìä Archivos generados:"
    
    # Contar archivos JSON generados
    json_count=$(find tools_data -name "*.json" 2>/dev/null | wc -l)
    csv_count=$(find category_data -name "*.csv" 2>/dev/null | wc -l)
    
    echo "   - Tools JSON: ${json_count} archivos en tools_data/"
    echo "   - Category CSV: ${csv_count} archivos en category_data/"
    
    if [ $json_count -gt 0 ]; then
        echo ""
        echo "üìÅ Ejemplos de archivos generados:"
        find tools_data -name "*.json" | head -3 | while read file; do
            echo "   - $file"
        done
    fi
    
    if [ $csv_count -gt 0 ]; then
        echo ""
        echo "üìä CSVs de categor√≠a generados:"
        find category_data -name "*.csv" | head -3 | while read file; do
            echo "   - $file"
        done
    fi
    
    echo ""
    echo "üìù Para m√°s categor√≠as, ejecuta:"
    echo "   python3 scrape.py --category 'Nombre de Categor√≠a'"
    echo ""
    echo "üîß Para configurar APIs y mejorar funcionalidad:"
    echo "   - Edita .env y agrega FETCHFOX_API_KEY"
    echo "   - Edita .env y agrega OPENROUTER_API_KEY"
    
else
    echo ""
    echo "‚ùå Error durante el scraping"
    echo "   Revisa los logs para m√°s detalles"
    echo "   Puedes intentar con menor concurrencia:"
    echo "   python3 scrape.py --category 'AI Personal Assistant Tools' --max-workers 2"
fi
