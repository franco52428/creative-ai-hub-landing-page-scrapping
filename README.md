# ğŸ¤– Enhanced Futurepedia AI Tools Scraper

Sistema profesional de scraping para herramientas de IA desde Futurepedia.io con capacidades avanzadas de procesamiento, traducciones automÃ¡ticas y arquitectura hÃ­brida FetchFox + BeautifulSoup.

## âœ¨ CaracterÃ­sticas Profesionales

### ğŸ”¥ **Arquitectura HÃ­brida Avanzada**
- **FetchFox API**: Scraping principal con alta eficiencia
- **BeautifulSoup Fallback**: Respaldo automÃ¡tico para mÃ¡xima robustez
- **Concurrent Processing**: Hasta 12 workers paralelos por categorÃ­a
- **Smart Retry Logic**: Sistema inteligente de reintentos con backoff

### ğŸŒ **Traducciones AI-Powered**
- **OpenRouter Integration**: Traducciones automÃ¡ticas con DeepSeek
- **5 Idiomas Soportados**: EspaÃ±ol, FrancÃ©s, AlemÃ¡n, PortuguÃ©s, Italiano  
- **PreservaciÃ³n de Contexto**: Mantiene terminologÃ­a tÃ©cnica correcta
- **Rate Limiting Inteligente**: GestiÃ³n automÃ¡tica de lÃ­mites API

### ğŸ“Š **Procesamiento Batch Profesional**
- **Modo `--all`**: Procesamiento automÃ¡tico de todas las categorÃ­as
- **CSV-Driven**: ConfiguraciÃ³n basada en archivo CSV centralizado
- **Progress Tracking**: Progreso detallado `[X/14]` categorÃ­as
- **Error Recovery**: ContinÃºa con siguiente categorÃ­a si una falla

### ğŸ—‚ï¸ **GestiÃ³n de Datos Inteligente**
- **DetecciÃ³n de Duplicados**: Evita re-scraping de herramientas existentes
- **Resume Capability**: Reanuda desde Ãºltima herramienta procesada
- **Structured Output**: JSON individual + CSV consolidado por categorÃ­a
- **Smart Pagination**: DetecciÃ³n automÃ¡tica de pÃ¡ginas con contenido

## ğŸ—ï¸ Estructura del Proyecto

```
scraping/
â”œâ”€â”€ ğŸ“ scrapers/
â”‚   â”œâ”€â”€ futurepedia_scraper.py   # Enhanced scraper engine
â”‚   â””â”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ ğŸ“ outputs/                  # Generated data
â”‚   â”œâ”€â”€ json/                    # Individual tool files
â”‚   â””â”€â”€ csv/                     # Category consolidated data  
â”œâ”€â”€ ğŸ“ .github/                  # Project documentation
â”‚   â””â”€â”€ instructions/            # Architecture & rules
â”œâ”€â”€ ğŸ”§ scrape.py                 # CLI orchestrator
â”œâ”€â”€ ğŸš€ run_scraping.sh           # Professional execution script
â”œâ”€â”€ ğŸ“‹ futurepeida_io_categories.csv  # Category URLs
â”œâ”€â”€ âš™ï¸ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ” .env                      # API keys configuration
â””â”€â”€ ğŸ“– README.md                 # This file
```

## ï¿½ï¸ Migraciones TypeORM

El proyecto incluye un sistema profesional para generar migraciones TypeORM a partir de los datos scrapeados:

```bash
# Generar todas las migraciones
./generate_migrations.sh
```

**Estructura de salida:**
```
migrations/
â”œâ”€â”€ tools_migrations/         # Archivos .ts de migraciones
â”œâ”€â”€ generate_migrations.py   # Generador principal
â”œâ”€â”€ validate_migrations.py   # Validador
â””â”€â”€ README.md                # DocumentaciÃ³n tÃ©cnica
```

Ver documentaciÃ³n completa en: [`migrations/README.md`](migrations/README.md)

## ï¿½ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. **Clonar el Repositorio**
```bash
git clone <repository-url>
cd scraping
```

### 2. **Instalar Dependencias**
```bash
pip install -r requirements.txt
```

### 3. **Configurar Variables de Entorno**
Crear archivo `.env`:
```bash
# APIs requeridas
FETCHFOX_API_KEY=tu_clave_fetchfox_aqui
OPENROUTER_API_KEY=tu_clave_openrouter_aqui

# ConfiguraciÃ³n opcional
ENABLE_FETCHFOX=true
ENABLE_TRANSLATIONS=true
TARGET_LANGUAGES=es,fr,de,pt,it
DEFAULT_MAX_WORKERS=8
DEFAULT_REQUEST_DELAY=2
DEFAULT_TIMEOUT=30
```

### 4. **Obtener API Keys**

#### **FetchFox API** (Scraping Principal)
- Registro: [fetchfoxai.com](https://fetchfoxai.com)
- Plan gratuito: 1,000 requests/mes
- Plan Pro: $20/mes para proyectos escalables

#### **OpenRouter API** (Traducciones AI)
- Registro: [openrouter.ai](https://openrouter.ai)  
- CrÃ©ditos gratuitos al registro
- Modelo usado: `deepseek/deepseek-chat` (econÃ³mico y eficiente)

## ğŸ¯ Uso Profesional

### **Comando BÃ¡sico - Una CategorÃ­a**
```bash
python3 scrape.py --category "https://www.futurepedia.io/ai-tools/personal-assistant" --max-workers 8
```

### **Comando Profesional - Todas las CategorÃ­as**
```bash
python3 scrape.py --all --csv futurepeida_io_categories.csv --max-workers 12
```

### **Script de EjecuciÃ³n AutomÃ¡tica**
```bash
./run_scraping.sh
```

### **Opciones Disponibles**
```bash
python3 scrape.py --help

Options:
  --category CATEGORY       URL de categorÃ­a especÃ­fica a scrapear
  --all                     Procesar todas las categorÃ­as del CSV
  --csv CSV                 Archivo CSV de categorÃ­as (default: futurepeida_io_categories.csv)
  --max-workers MAX_WORKERS Hilos paralelos por categorÃ­a (default: 4)
```

## ğŸ“ˆ Capacidades y Rendimiento

### **Volumen de Datos**
- **14+ categorÃ­as** disponibles en CSV
- **~200-250 herramientas** por categorÃ­a (promedio)
- **Total estimado: ~3,000 herramientas**
- **Tiempo estimado: 2-3 horas** (modo profesional completo)

### **Rendimiento Optimizado**
- **Concurrent Processing**: 12 workers simultÃ¡neos
- **Smart Caching**: Evita re-scraping de datos existentes
- **Pagination Detection**: AutomÃ¡tica hasta final de contenido
- **Error Recovery**: ContinÃºa automÃ¡ticamente ante fallos temporales

### **Ejemplo de Rendimiento Real**
```
âœ… CategorÃ­a: AI Personal Assistant Tools
   - 215 herramientas scrapeadas
   - Tiempo: 4.5 minutos
   - Workers: 8 paralelos
   - Success rate: 100%
```

## ğŸ—ƒï¸ Estructura de Datos

### **Output JSON (por herramienta)**
```json
{
    "name": "Tool Name",
    "url": "https://www.futurepedia.io/tool/tool-name",
    "description": "Tool description...",
    "category": "personal-assistant",
    "pricing": "Free/Paid/Freemium", 
    "features": ["feature1", "feature2"],
    "translations": {
        "es": { "name": "Nombre", "description": "DescripciÃ³n..." },
        "fr": { "name": "Nom", "description": "Description..." }
    },
    "scraped_at": "2025-08-29T10:30:00Z"
}
```

### **Output CSV (por categorÃ­a)**  
```csv
name,url,description,category,pricing,features,scraped_at
Tool Name,https://...,Description...,personal-assistant,Free,"feature1,feature2",2025-08-29T10:30:00Z
```

## ğŸ”§ Arquitectura TÃ©cnica

### **Hybrid Scraping Strategy**
1. **Primary**: FetchFox API para requests complejos
2. **Fallback**: BeautifulSoup para mÃ¡xima compatibilidad  
3. **Smart Detection**: AutomÃ¡tica segÃºn response y disponibilidad

### **AI Translation Pipeline**  
1. **Content Extraction**: Texto base en inglÃ©s
2. **OpenRouter Processing**: DeepSeek model para traducciones
3. **Context Preservation**: Mantiene terminologÃ­a tÃ©cnica
4. **Quality Assurance**: ValidaciÃ³n automÃ¡tica de output

### **Concurrent Architecture**
```python
# Ejemplo de configuraciÃ³n interna
ThreadPoolExecutor(max_workers=12) 
â”œâ”€â”€ Tool 1 Scraping + Translation
â”œâ”€â”€ Tool 2 Scraping + Translation  
â”œâ”€â”€ Tool 3 Scraping + Translation
â””â”€â”€ ... (hasta 12 simultÃ¡neos)
```

## ğŸ“Š Monitoreo y Logs

### **Logging Estructurado**
```bash
tail -f scraping.log
```

### **MÃ©tricas en Tiempo Real**
- Herramientas procesadas por minuto
- Success/failure rates por categorÃ­a  
- Tiempo estimado de finalizaciÃ³n
- Usage de APIs (FetchFox + OpenRouter)

### **Ejemplo de Log Output**
```
2025-08-29 10:30:15 | INFO | [1/14] Procesando: AI Personal Assistant Tools
2025-08-29 10:30:16 | INFO | 215 tools encontradas | a procesar: 200 | ya vistas: 15
2025-08-29 10:32:45 | INFO | âœ… CategorÃ­a completada: 215 herramientas en 2.5 min
2025-08-29 10:32:46 | INFO | [2/14] Procesando: Research Assistant Tools...
```

## ğŸ› ï¸ SoluciÃ³n de Problemas

### **Errores Comunes**

#### **API Key Missing**
```bash
Error: FETCHFOX_API_KEY not found in environment
```
**SoluciÃ³n**: Verificar archivo `.env` con las API keys correctas

#### **Rate Limit Exceeded**  
```bash
Warning: OpenRouter rate limit hit, retrying in 60s...
```
**SoluciÃ³n**: El sistema maneja automÃ¡ticamente con backoff inteligente

#### **Connection Timeout**
```bash
Error: Timeout connecting to Futurepedia
```
**SoluciÃ³n**: Verificar conexiÃ³n a internet, el sistema reintentarÃ¡ automÃ¡ticamente

### **OptimizaciÃ³n de Rendimiento**

#### **Para Proyectos Grandes**
```bash
# Reducir workers si hay timeouts
python3 scrape.py --all --max-workers 6

# Para conexiones lentas  
python3 scrape.py --all --max-workers 4
```

#### **Para MÃ¡ximo Rendimiento**
```bash
# ConfiguraciÃ³n agresiva (requiere APIs premium)
python3 scrape.py --all --max-workers 16
```

## ğŸ”„ Actualizaciones y Mantenimiento

### **Actualizar CategorÃ­as**
Editar `futurepeida_io_categories.csv`:
```csv
Category
https://www.futurepedia.io/ai-tools/nueva-categoria
```

### **Backup de Datos**
```bash
# Backup automÃ¡tico de outputs
tar -czf backup_$(date +%Y%m%d).tar.gz outputs/
```

### **Limpiar Cache**
```bash
# Limpiar datos antiguos para re-scraping completo
rm -rf outputs/json/* outputs/csv/*
```

## ğŸ“ Soporte y Contribuciones

### **Reportar Issues**
- Incluir logs relevantes de `scraping.log`
- Especificar configuraciÃ³n de `.env` (sin API keys)
- Describir comportamiento esperado vs actual

### **Mejoras Futuras Planificadas**
- [ ] Dashboard web para monitoreo en tiempo real
- [ ] Soporte para mÃ¡s sitios (ProductHunt, etc.)
- [ ] API REST para consulta de datos scrapeados
- [ ] ClasificaciÃ³n automÃ¡tica por tags ML
- [ ] IntegraciÃ³n con bases de datos (PostgreSQL/MongoDB)

---

## âš¡ Quick Start

```bash
# Setup completo en 3 comandos
pip install -r requirements.txt
cp .env.example .env  # Editar con tus API keys
./run_scraping.sh     # Ejecutar modo profesional
```

**ğŸ‰ Â¡Tu scraper profesional de herramientas AI estÃ¡ listo!**

## ğŸš€ InstalaciÃ³n

1. **Clonar el repositorio** (si corresponde) o navegar al directorio:
```bash
cd /home/jhoto/tools-hub-ai/scraping
```

2. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

3. **Configurar variables de entorno** (opcional):
```bash
# Las configuraciones por defecto estÃ¡n en .env
# Puedes modificar segÃºn tus necesidades
```

## ğŸ“– Uso

### Scraping de CategorÃ­as

#### 1. Scraping de una categorÃ­a especÃ­fica
```bash
python main.py --category "https://www.futurepedia.io/ai-tools/personal-assistant"
```

#### 2. Scraping de todas las categorÃ­as
```bash
python main.py --all
```

### GestiÃ³n de Datos

#### Ver estadÃ­sticas del scraping
```bash
python utils.py --stats
```

#### Generar CSV maestro con todas las herramientas
```bash
python utils.py --master-csv
```

#### Limpiar herramientas incompletas
```bash
python utils.py --clean
```

#### Listar todas las categorÃ­as encontradas
```bash
python utils.py --categories
```

## Estructura de Datos

Cada herramienta se almacena como un archivo JSON con la siguiente estructura:

```json
{
    "slug": "nombre-herramienta",
    "image_url": "https://example.com/image.jpg",
    "redirect_url": "https://herramienta-oficial.com",
    "demo_url": "https://demo.herramienta.com",
    "technical_requirements": "Navegador web moderno, conexiÃ³n a internet",
    "searchIndexEn": "tÃ©rminos de bÃºsqueda en inglÃ©s",
    "searchIndexEs": "tÃ©rminos de bÃºsqueda en espaÃ±ol",
    "translations": {
        "es": {
            "title": "Nombre de la Herramienta",
            "shortDescription": "DescripciÃ³n corta",
            "longDescription": "DescripciÃ³n detallada",
            "tags": "etiquetas, separadas, por, comas",
            "pricingInfo": "InformaciÃ³n de precios",
            "category": "categoria",
            "appType": "tipo-de-aplicacion"
        },
        "en": { ... },
        "pt": { ... },
        "fr": { ... },
        "de": { ... }
    }
}
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno (.env)

```bash
# ConfiguraciÃ³n de scraping
DELAY_BETWEEN_REQUESTS=2      # Segundos entre peticiones
MAX_RETRIES=3                 # MÃ¡ximo nÃºmero de reintentos
TIMEOUT=30                    # Timeout en segundos

# Directorios
TOOLS_DIR=tools               # Directorio para JSONs individuales
DATA_DIR=data                # Directorio para CSVs consolidados

# URLs base
BASE_URL=https://www.futurepedia.io
CATEGORY_BASE_URL=https://www.futurepedia.io/ai-tools

# PaginaciÃ³n
MAX_PAGES_PER_CATEGORY=50     # MÃ¡ximo pÃ¡ginas por categorÃ­a
ITEMS_PER_PAGE=20            # Items esperados por pÃ¡gina
```

## ğŸ“ˆ CategorÃ­as Disponibles

El archivo `futurepeida_io_categories.csv` contiene las categorÃ­as a procesar:

- AI Personal Assistant Tools
- Research Assistant Tools
- Spreadsheet Assistant Tools
- Translators
- Presentations
- Email Assistant
- Search Engine
- Prompt Generators
- Writing Generators
- Storyteller
- Summarizer
- Code Assistant
- No Code
- SQL Assistant

## ğŸš¦ Proceso de Scraping

1. **Carga de categorÃ­a**: Lee la URL de la categorÃ­a
2. **Descubrimiento de herramientas**: Navega por todas las pÃ¡ginas de la categorÃ­a
3. **ExtracciÃ³n de datos**: Para cada herramienta, extrae informaciÃ³n detallada
4. **Procesamiento de datos**: Aplica transformaciones y traducciones
5. **Almacenamiento**: Guarda JSON individual y actualiza CSV de categorÃ­a

## ğŸ“‹ Archivos Generados

### Por CategorÃ­a:
- `data/{categoria}_tools.csv` - CSV con todas las herramientas de la categorÃ­a
- `tools/{slug}.json` - JSON individual por cada herramienta

### Consolidados:
- `data/master_tools.csv` - CSV maestro con todas las herramientas
- `scraping.log` - Log detallado de todas las operaciones

## ğŸ› ï¸ CaracterÃ­sticas TÃ©cnicas

- **Manejo de errores robusto** con reintentos automÃ¡ticos
- **Rate limiting** para evitar bloqueos
- **User Agent rotation** para evitar detecciÃ³n
- **Logging estructurado** con correlaciÃ³n de eventos
- **ExtracciÃ³n flexible** que se adapta a cambios en el sitio
- **ValidaciÃ³n de datos** antes del almacenamiento

## ğŸ” Monitoreo y Logs

Los logs se guardan en `scraping.log` e incluyen:
- Inicio y fin de operaciones
- Errores y advertencias
- EstadÃ­sticas de progreso
- URLs procesadas
- Datos extraÃ­dos

Ejemplo de log:
```
2025-08-27 10:30:15,123 - INFO - Starting to scrape category: https://www.futurepedia.io/ai-tools/personal-assistant
2025-08-27 10:30:17,456 - INFO - Found 25 tools on page 1
2025-08-27 10:30:20,789 - INFO - Scraping tool 1/25: ChatGPT Assistant
2025-08-27 10:30:23,012 - INFO - Saved tool data to tools/chatgpt-assistant.json
```

## ğŸ“Š MÃ©tricas de Calidad

El sistema rastrea automÃ¡ticamente:
- NÃºmero de herramientas extraÃ­das por categorÃ­a
- Porcentaje de herramientas con imÃ¡genes
- Porcentaje de herramientas con URLs de redirecciÃ³n
- Herramientas con datos incompletos

## ğŸš¨ Consideraciones Importantes

### Respeto por el Sitio Web
- Delays configurables entre peticiones
- User agents realistas
- Respeto por robots.txt
- Monitoreo de cÃ³digos de respuesta

### Calidad de Datos
- ValidaciÃ³n automÃ¡tica de campos requeridos
- Limpieza de datos inconsistentes
- DetecciÃ³n de duplicados
- VerificaciÃ³n de URLs

### Escalabilidad
- Arquitectura modular para agregar nuevos sitios
- Procesamiento por lotes
- Almacenamiento eficiente
- Capacidad de resumir scraping interrumpido

## ğŸ”„ PrÃ³ximos Pasos

1. **Completar Futurepedia**: Procesar todas las categorÃ­as disponibles
2. **Agregar mÃ¡s sitios**: Integrar otros directorios de IA
3. **Mejorar traducciones**: Implementar servicio de traducciÃ³n automÃ¡tica
4. **Sistema de ranking**: Integrar sistema de puntuaciÃ³n y comentarios
5. **API**: Crear API REST para acceso a los datos
6. **Dashboard**: Interfaz web para gestiÃ³n y visualizaciÃ³n

## ğŸ¤ Contribuciones

Para agregar nuevos scrapers o mejorar los existentes:

1. Crear nuevo scraper en `scrapers/`
2. Seguir la interfaz establecida
3. Actualizar configuraciÃ³n en `config/settings.py`
4. Agregar tests y documentaciÃ³n
5. Actualizar este README

## ğŸ“ Soporte

Para problemas o mejoras, revisar:
- Logs en `scraping.log`
- ConfiguraciÃ³n en `.env`
- EstadÃ­sticas con `python utils.py --stats`

---

**Ãšltima actualizaciÃ³n**: 27 de Agosto, 2025
**VersiÃ³n**: 1.0.0
**Estado**: ProducciÃ³n - Primera categorÃ­a lista para scraping
